{"id":"cc90bbe5-2ac0-46d2-b5f5-2c92a07d87ef","data":{"nodes":[{"id":"GoogleGenerativeAIModel-CS0W8","type":"genericNode","position":{"x":2340.847503609555,"y":1275.7611409417955},"data":{"type":"GoogleGenerativeAIModel","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"Google Generative AI\"\n    description = \"Generate text using Google Generative AI.\"\n    icon = \"GoogleGenerativeAI\"\n    name = \"GoogleGenerativeAIModel\"\n\n    inputs = LCModelComponent._base_inputs + [\n        IntInput(\n            name=\"max_output_tokens\",\n            display_name=\"Max Output Tokens\",\n            info=\"The maximum number of tokens to generate.\",\n        ),\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=[\"gemini-1.5-pro\", \"gemini-1.5-flash\", \"gemini-1.0-pro\", \"gemini-1.0-pro-vision\"],\n            value=\"gemini-1.5-pro\",\n        ),\n        SecretStrInput(\n            name=\"google_api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError:\n            raise ImportError(\"The 'langchain_google_genai' package is required to use the Google Generative AI model.\")\n\n        google_api_key = self.google_api_key\n        model = self.model\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        output = ChatGoogleGenerativeAI(  # type: ignore\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key),\n        )\n\n        return output  # type: ignore\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"google_api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"value":"","name":"google_api_key","display_name":"Google API Key","advanced":false,"input_types":[],"dynamic":false,"info":"The Google API Key to use for the Google Generative AI.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"max_output_tokens":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"8192","name":"max_output_tokens","display_name":"Max Output Tokens","advanced":false,"dynamic":false,"info":"The maximum number of tokens to generate.","title_case":false,"type":"int","_input_type":"IntInput"},"model":{"trace_as_metadata":true,"options":["gemini-1.5-pro","gemini-1.5-flash","gemini-1.0-pro","gemini-1.0-pro-vision"],"combobox":false,"required":false,"placeholder":"","show":true,"value":"gemini-1.5-flash","name":"model","display_name":"Model","advanced":false,"dynamic":false,"info":"The name of the model to use.","title_case":false,"type":"str","_input_type":"DropdownInput"},"n":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"n","display_name":"N","advanced":true,"dynamic":false,"info":"Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"stream","display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"system_message","display_name":"System Message","advanced":false,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"0.0","name":"temperature","display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"},"top_k":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"top_k","display_name":"Top K","advanced":true,"dynamic":false,"info":"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.","title_case":false,"type":"int","_input_type":"IntInput"},"top_p":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":".95","name":"top_p","display_name":"Top P","advanced":false,"dynamic":false,"info":"The maximum cumulative probability of tokens to consider when sampling.","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generate text using Google Generative AI.","icon":"GoogleGenerativeAI","base_classes":["LanguageModel","Message"],"display_name":"Google Generative AI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","system_message","stream","max_output_tokens","model","google_api_key","top_p","temperature","n","top_k"],"beta":false,"edited":false},"id":"GoogleGenerativeAIModel-CS0W8"},"selected":false,"width":384,"height":859,"dragging":false,"positionAbsolute":{"x":2340.847503609555,"y":1275.7611409417955}},{"id":"TextOutput-d9nFE","type":"genericNode","position":{"x":2896.9291358458267,"y":1880.9626202552622},"data":{"type":"TextOutput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextOutputComponent(TextComponent):\n    display_name = \"Text Output\"\n    description = \"Display a text output in the Playground.\"\n    icon = \"type\"\n    name = \"TextOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as output.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        self.status = self.input_value\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as output.","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Display a text output in the Playground.","icon":"type","base_classes":["Message"],"display_name":"Text Output","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"edited":false},"id":"TextOutput-d9nFE"},"selected":false,"width":384,"height":298,"positionAbsolute":{"x":2896.9291358458267,"y":1880.9626202552622},"dragging":false},{"id":"Prompt-Y2bvA","type":"genericNode","position":{"x":1726.1040189342011,"y":1696.1116782972658},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"You are the top editor at a company that creates digests of research papers, similar to what Readers' Digest does with books.\n\nAfter the title, include the author names, the publication name and year of publication, and any \nhyperlink to the original paper. If no hyperlink is provided, include unique identifying information \nsuch as DOI, arXiv, PMID, etc.\n\nFollow with a digest of the research paper, being sure to include all sections and sub-sections in the \noriginal paper, including the abstract but exclude appendixes and bibliography. \n\nThe writing should be in the same style as the original authors; imagine their editor gave them the task to \ncondense the paper down to 10% of the original size, and that is the sort of digest you are creating.\n\nExample:\n----\n# This is the Title of an Amazing Research Paper\n\nAuthors: Alice Jones, Bob Smith\n\nPublication: The Research Journal (2023)\n\nURL: https://doi.org/10.1186/s1234-566-032-3\n\n[digest of research paper]\n\n----\n\nOutput should be in English (regardless of the paper's language), and in Markdown format.\n","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput","load_from_db":false}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","base_classes":["Message"],"display_name":"Prompt","documentation":"","custom_fields":{"template":[]},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"edited":false},"id":"Prompt-Y2bvA","description":"Create a prompt template with dynamic variables.","display_name":"Prompt"},"selected":false,"width":384,"height":326},{"id":"Prompt-JBx0Z","type":"genericNode","position":{"x":1709.9593707440326,"y":1161.4212576598093},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"Parsed Research Paper:\n\n{paper_text}","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput","load_from_db":false},"paper_text":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"paper_text","display_name":"paper_text","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","base_classes":["Message"],"display_name":"Prompt","documentation":"","custom_fields":{"template":["paper_text"]},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"edited":false},"id":"Prompt-JBx0Z","description":"Create a prompt template with dynamic variables.","display_name":"Prompt"},"selected":false,"width":384,"height":412},{"id":"ParseData-7fnJC","type":"genericNode","position":{"x":1201.1329167993918,"y":1112.6108635262467},"data":{"type":"ParseData","node":{"template":{"_type":"Component","data":{"trace_as_input":true,"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"data","display_name":"Data","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The data to convert to text.","title_case":false,"type":"other","_input_type":"DataInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.io import DataInput, MultilineInput, Output, StrInput\nfrom langflow.schema.message import Message\n\n\nclass ParseDataComponent(Component):\n    display_name = \"Parse Data\"\n    description = \"Convert Data into plain text following a specified template.\"\n    icon = \"braces\"\n    name = \"ParseData\"\n\n    inputs = [\n        DataInput(name=\"data\", display_name=\"Data\", info=\"The data to convert to text.\"),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.\",\n            value=\"{text}\",\n        ),\n        StrInput(name=\"sep\", display_name=\"Separator\", advanced=True, value=\"\\n\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"parse_data\"),\n    ]\n\n    def parse_data(self) -> Message:\n        data = self.data if isinstance(self.data, list) else [self.data]\n        template = self.template\n\n        result_string = data_to_text(template, data, sep=self.sep)\n        self.status = result_string\n        return Message(text=result_string)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"sep":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"\n","name":"sep","display_name":"Separator","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"template":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"{text}","name":"template","display_name":"Template","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Convert Data into plain text following a specified template.","icon":"braces","base_classes":["Message"],"display_name":"Parse Data","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"parse_data","value":"__UNDEFINED__","cache":true}],"field_order":["data","template","sep"],"beta":false,"edited":false},"id":"ParseData-7fnJC"},"selected":false,"width":384,"height":374},{"id":"base64 File-k0XGQ","type":"genericNode","position":{"x":662.4798684352011,"y":992.1170697179748},"data":{"type":"base64 File","node":{"template":{"_type":"Component","base64_string":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":true,"placeholder":"Enter base64 encoded file string here...","show":true,"value":"","name":"base64_string","display_name":"base64 Encoded File","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import base64\r\nimport os\r\nimport tempfile\r\nfrom langflow.custom import Component\r\nfrom langflow.schema import Data\r\nfrom langflow.base.data.utils import parse_text_file_to_data\r\nfrom langflow.io import BoolInput, MessageTextInput, Output\r\n\r\nclass Base64FileLoader(Component):\r\n    display_name = \"base64 File\"\r\n    description = \"Loads a file from a base64 encoded string, temporarily stores it, and processes similar to FileComponent.\"\r\n    icon = \"file-text\"\r\n    name = \"base64 File\"\r\n\r\n    inputs = [\r\n        MessageTextInput(name=\"base64_string\", display_name=\"base64 Encoded File\", required=True, placeholder=\"Enter base64 encoded file string here...\"),\r\n        MessageTextInput(name=\"filename\", display_name=\"Filename\", required=True, placeholder=\"Enter filename with extension...\"),\r\n        BoolInput(\r\n            name=\"silent_errors\",\r\n            display_name=\"Silent Errors\",\r\n            advanced=True,\r\n            info=\"If true, errors will not raise an exception.\",\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(display_name=\"Data\", name=\"data\", method=\"decode_and_load_file\"),\r\n    ]\r\n\r\n    def decode_and_load_file(self) -> Data:\r\n\r\n        if not self.base64_string:\r\n            raise ValueError(\"Base64 string is empty. Please provide a valid base64 encoded file.\")\r\n        if not self.filename:\r\n            raise ValueError(\"Filename is required.\")\r\n\r\n        base64_val = self.base64_string\r\n        filename_val = self.filename \r\n        \r\n        # Decode the base64 string to bytes\r\n        try:\r\n            file_bytes = base64.b64decode(base64_val)\r\n        except Exception as e:\r\n            raise ValueError(f\"Failed to decode base64 string: {str(e)}\")\r\n\r\n        # Create a temporary directory to store the file\r\n        with tempfile.TemporaryDirectory() as tmpdirname:\r\n            temp_file_path = os.path.join(tmpdirname, filename_val)\r\n\r\n            # Write bytes to a temporary file\r\n            with open(temp_file_path, 'wb') as temp_file:\r\n                temp_file.write(file_bytes)\r\n\r\n            # Use the existing utility to parse the text file\r\n            try:\r\n                data = parse_text_file_to_data(temp_file_path, silent_errors=self.silent_errors)\r\n            except Exception as e:\r\n                raise ValueError(f\"Failed to parse the file: {str(e)}\")\r\n\r\n            # Ensure file is deleted after processing if required\r\n            os.remove(temp_file_path)\r\n\r\n            self.status = \"File processed successfully.\"\r\n            return data\r\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"filename":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":true,"placeholder":"Enter filename with extension...","show":true,"value":"","name":"filename","display_name":"Filename","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"},"silent_errors":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"silent_errors","display_name":"Silent Errors","advanced":true,"dynamic":false,"info":"If true, errors will not raise an exception.","title_case":false,"type":"bool","_input_type":"BoolInput"}},"description":"Loads a file from a base64 encoded string, temporarily stores it, and processes similar to FileComponent.","icon":"file-text","base_classes":["Data"],"display_name":"base64 File","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"data","display_name":"Data","method":"decode_and_load_file","value":"__UNDEFINED__","cache":true}],"field_order":["base64_string","filename","silent_errors"],"beta":false,"edited":true,"official":false},"id":"base64 File-k0XGQ"},"selected":false,"width":384,"height":440,"positionAbsolute":{"x":662.4798684352011,"y":992.1170697179748},"dragging":false},{"id":"ParameterInput-AtPx5","type":"genericNode","position":{"x":68.92986503033467,"y":901.1480049376594},"data":{"type":"ParameterInput","node":{"template":{"_type":"Component","base64_file":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"base64_file","value":"","display_name":"base64 File","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Contents of file with base64 encoding","title_case":false,"type":"str","_input_type":"MessageTextInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\n\n# This should work from v1.0.16 only which fixes issue #3380\n\n# It is admittedly fairly hack-ey but demonstrates the concept. \n# Developer will need to edit the component to name parameters sensibly,\n# and to add more parameters as necessary!\n\nclass ParameterComponent(Component):\n    # The tweak can use the display_name, e.g.\n    #   \"tweaks\": {\"Parameter Input\": { \"parameter_1\": varParameter1, \"parameter_2\": varParameter2 }}\n    # As this will not change on export/import\n    display_name = \"Parameter Input\"\n    description = \"Parameters combined into single Component.\"\n    icon = \"type\"\n    name = \"ParameterInput\"\n\n    # Each input will be a MessageTextInput with a distinct name=\n    # There will be a corresponding output with a name= using _out postfix\n    # There will be a corresponding function f_ for each parameter to define the output\n    inputs = [\n        MessageTextInput(\n            name=\"base64_file\",\n            display_name=\"base64 File\",\n            info=\"Contents of file with base64 encoding\",\n        ),\n        MessageTextInput(\n            name=\"filename\",\n            display_name=\"Filename\",\n            info=\"Name of file with extension\",\n        ),\n    ]\n\n    outputs = [\n        # name= and method= parameters need to not conflict with the input name=: \n        #   Name of this is the parameter name with _out\n        Output(display_name=\"base64 File\", name=\"base64_file_out\", method=\"f_base64_file\"),\n        Output(display_name=\"Filename\", name=\"filename_out\", method=\"f_filename\"),\n    ]\n\n    def f_base64_file(self) -> Message:\n        return self.base64_file\n\n    def f_filename(self) -> Message:\n        return self.filename\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"filename":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"filename","value":"","display_name":"Filename","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Name of file with extension","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Parameters combined into single Component.","icon":"type","base_classes":["Message"],"display_name":"Parameter Input","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"base64_file_out","display_name":"base64 File","method":"f_base64_file","value":"__UNDEFINED__","cache":true},{"types":["Message"],"selected":"Message","name":"filename_out","display_name":"Filename","method":"f_filename","value":"__UNDEFINED__","cache":true}],"field_order":["base64_file","filename"],"beta":false,"edited":true},"id":"ParameterInput-AtPx5"},"selected":false,"width":384,"height":429,"positionAbsolute":{"x":68.92986503033467,"y":901.1480049376594},"dragging":false}],"edges":[{"source":"GoogleGenerativeAIModel-CS0W8","target":"TextOutput-d9nFE","sourceHandle":"{œdataTypeœ:œGoogleGenerativeAIModelœ,œidœ:œGoogleGenerativeAIModel-CS0W8œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œTextOutput-d9nFEœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","id":"reactflow__edge-GoogleGenerativeAIModel-CS0W8{œdataTypeœ:œGoogleGenerativeAIModelœ,œidœ:œGoogleGenerativeAIModel-CS0W8œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-TextOutput-d9nFE{œfieldNameœ:œinput_valueœ,œidœ:œTextOutput-d9nFEœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"TextOutput-d9nFE","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"GoogleGenerativeAIModel","id":"GoogleGenerativeAIModel-CS0W8","name":"text_output","output_types":["Message"]}},"selected":false,"className":""},{"source":"Prompt-Y2bvA","target":"GoogleGenerativeAIModel-CS0W8","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-Y2bvAœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-CS0W8œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","id":"reactflow__edge-Prompt-Y2bvA{œdataTypeœ:œPromptœ,œidœ:œPrompt-Y2bvAœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-CS0W8{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-CS0W8œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"system_message","id":"GoogleGenerativeAIModel-CS0W8","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-Y2bvA","name":"prompt","output_types":["Message"]}},"selected":false,"className":""},{"source":"Prompt-JBx0Z","target":"GoogleGenerativeAIModel-CS0W8","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-JBx0Zœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-CS0W8œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","id":"reactflow__edge-Prompt-JBx0Z{œdataTypeœ:œPromptœ,œidœ:œPrompt-JBx0Zœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-CS0W8{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-CS0W8œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"GoogleGenerativeAIModel-CS0W8","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-JBx0Z","name":"prompt","output_types":["Message"]}},"selected":false,"className":""},{"source":"ParseData-7fnJC","target":"Prompt-JBx0Z","sourceHandle":"{œdataTypeœ:œParseDataœ,œidœ:œParseData-7fnJCœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œpaper_textœ,œidœ:œPrompt-JBx0Zœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","id":"reactflow__edge-ParseData-7fnJC{œdataTypeœ:œParseDataœ,œidœ:œParseData-7fnJCœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-JBx0Z{œfieldNameœ:œpaper_textœ,œidœ:œPrompt-JBx0Zœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"paper_text","id":"Prompt-JBx0Z","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"ParseData","id":"ParseData-7fnJC","name":"text","output_types":["Message"]}},"selected":false,"className":""},{"source":"base64 File-k0XGQ","sourceHandle":"{œdataTypeœ:œbase64 Fileœ,œidœ:œbase64 File-k0XGQœ,œnameœ:œdataœ,œoutput_typesœ:[œDataœ]}","target":"ParseData-7fnJC","targetHandle":"{œfieldNameœ:œdataœ,œidœ:œParseData-7fnJCœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"data","id":"ParseData-7fnJC","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"base64 File","id":"base64 File-k0XGQ","name":"data","output_types":["Data"]}},"id":"reactflow__edge-base64 File-k0XGQ{œdataTypeœ:œbase64 Fileœ,œidœ:œbase64 File-k0XGQœ,œnameœ:œdataœ,œoutput_typesœ:[œDataœ]}-ParseData-7fnJC{œfieldNameœ:œdataœ,œidœ:œParseData-7fnJCœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","className":""},{"source":"ParameterInput-AtPx5","sourceHandle":"{œdataTypeœ:œParameterInputœ,œidœ:œParameterInput-AtPx5œ,œnameœ:œbase64_file_outœ,œoutput_typesœ:[œMessageœ]}","target":"base64 File-k0XGQ","targetHandle":"{œfieldNameœ:œbase64_stringœ,œidœ:œbase64 File-k0XGQœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"base64_string","id":"base64 File-k0XGQ","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"ParameterInput","id":"ParameterInput-AtPx5","name":"base64_file_out","output_types":["Message"]}},"id":"reactflow__edge-ParameterInput-AtPx5{œdataTypeœ:œParameterInputœ,œidœ:œParameterInput-AtPx5œ,œnameœ:œbase64_file_outœ,œoutput_typesœ:[œMessageœ]}-base64 File-k0XGQ{œfieldNameœ:œbase64_stringœ,œidœ:œbase64 File-k0XGQœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":""},{"source":"ParameterInput-AtPx5","sourceHandle":"{œdataTypeœ:œParameterInputœ,œidœ:œParameterInput-AtPx5œ,œnameœ:œfilename_outœ,œoutput_typesœ:[œMessageœ]}","target":"base64 File-k0XGQ","targetHandle":"{œfieldNameœ:œfilenameœ,œidœ:œbase64 File-k0XGQœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"filename","id":"base64 File-k0XGQ","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"ParameterInput","id":"ParameterInput-AtPx5","name":"filename_out","output_types":["Message"]}},"id":"reactflow__edge-ParameterInput-AtPx5{œdataTypeœ:œParameterInputœ,œidœ:œParameterInput-AtPx5œ,œnameœ:œfilename_outœ,œoutput_typesœ:[œMessageœ]}-base64 File-k0XGQ{œfieldNameœ:œfilenameœ,œidœ:œbase64 File-k0XGQœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":""}],"viewport":{"x":-26.47675109801719,"y":-372.27033564622707,"zoom":0.6597539553864481}},"description":"Creates a digest version of a research paper","name":"ResearchPaperDigest","last_tested_version":"1.0.17","endpoint_name":"research-paper-digest","is_component":false}