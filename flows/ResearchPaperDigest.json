{"id":"1d6e70c6-ffea-476b-8033-c0227b4c8d53","data":{"nodes":[{"id":"GoogleGenerativeAIModel-YZ8QZ","type":"genericNode","position":{"x":2340.847503609555,"y":1275.7611409417955},"data":{"type":"GoogleGenerativeAIModel","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"Google Generative AI\"\n    description = \"Generate text using Google Generative AI.\"\n    icon = \"GoogleGenerativeAI\"\n    name = \"GoogleGenerativeAIModel\"\n\n    inputs = LCModelComponent._base_inputs + [\n        IntInput(\n            name=\"max_output_tokens\",\n            display_name=\"Max Output Tokens\",\n            info=\"The maximum number of tokens to generate.\",\n        ),\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=[\"gemini-1.5-pro\", \"gemini-1.5-flash\", \"gemini-1.0-pro\", \"gemini-1.0-pro-vision\"],\n            value=\"gemini-1.5-pro\",\n        ),\n        SecretStrInput(\n            name=\"google_api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError:\n            raise ImportError(\"The 'langchain_google_genai' package is required to use the Google Generative AI model.\")\n\n        google_api_key = self.google_api_key\n        model = self.model\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        output = ChatGoogleGenerativeAI(  # type: ignore\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key),\n        )\n\n        return output  # type: ignore\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"google_api_key":{"load_from_db":true,"required":false,"placeholder":"","show":true,"value":"","name":"google_api_key","display_name":"Google API Key","advanced":false,"input_types":[],"dynamic":false,"info":"The Google API Key to use for the Google Generative AI.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"max_output_tokens":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"8192","name":"max_output_tokens","display_name":"Max Output Tokens","advanced":false,"dynamic":false,"info":"The maximum number of tokens to generate.","title_case":false,"type":"int","_input_type":"IntInput"},"model":{"trace_as_metadata":true,"options":["gemini-1.5-pro","gemini-1.5-flash","gemini-1.0-pro","gemini-1.0-pro-vision"],"combobox":false,"required":false,"placeholder":"","show":true,"value":"gemini-1.5-flash","name":"model","display_name":"Model","advanced":false,"dynamic":false,"info":"The name of the model to use.","title_case":false,"type":"str","_input_type":"DropdownInput"},"n":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"n","display_name":"N","advanced":true,"dynamic":false,"info":"Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"stream","display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"system_message","display_name":"System Message","advanced":false,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"0.0","name":"temperature","display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"},"top_k":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"top_k","display_name":"Top K","advanced":true,"dynamic":false,"info":"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.","title_case":false,"type":"int","_input_type":"IntInput"},"top_p":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":".95","name":"top_p","display_name":"Top P","advanced":false,"dynamic":false,"info":"The maximum cumulative probability of tokens to consider when sampling.","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generate text using Google Generative AI.","icon":"GoogleGenerativeAI","base_classes":["LanguageModel","Message"],"display_name":"Google Generative AI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","system_message","stream","max_output_tokens","model","google_api_key","top_p","temperature","n","top_k"],"beta":false,"edited":false},"id":"GoogleGenerativeAIModel-YZ8QZ"},"selected":true,"width":384,"height":903,"dragging":false,"positionAbsolute":{"x":2340.847503609555,"y":1275.7611409417955}},{"id":"TextOutput-vdHQL","type":"genericNode","position":{"x":2896.9291358458267,"y":1880.9626202552622},"data":{"type":"TextOutput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextOutputComponent(TextComponent):\n    display_name = \"Text Output\"\n    description = \"Display a text output in the Playground.\"\n    icon = \"type\"\n    name = \"TextOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as output.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        self.status = self.input_value\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as output.","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Display a text output in the Playground.","icon":"type","base_classes":["Message"],"display_name":"Text Output","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"edited":false},"id":"TextOutput-vdHQL"},"selected":false,"width":384,"height":317,"positionAbsolute":{"x":2896.9291358458267,"y":1880.9626202552622},"dragging":false},{"id":"Prompt-BmJjb","type":"genericNode","position":{"x":1726.1040189342011,"y":1696.1116782972658},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":"You are the top editor at a company that creates digests of research papers, similar to what Readers' Digest does with books.\n\nAfter the title, include the author names, the publication name and year of publication, and any \nhyperlink to the original paper. If no hyperlink is provided, include unique identifying information \nsuch as DOI, arXiv, PMID, etc.\n\nFollow with a digest of the research paper, being sure to include all sections and sub-sections in the \noriginal paper, including the abstract but exclude appendixes and bibliography. \n\nThe writing should be in the same style as the original authors; imagine their editor gave them the task to \ncondense the paper down to 10% of the original size, and that is the sort of digest you are creating.\n\nExample:\n----\n# This is the Title of an Amazing Research Paper\n\nAuthors: Alice Jones, Bob Smith\n\nPublication: The Research Journal (2023)\n\nURL: https://doi.org/10.1186/s1234-566-032-3\n\n[digest of research paper]\n\n----\n\nOutput should be in English (regardless of the paper's language), and in Markdown format.\n","name":"template","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":[]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false},"id":"Prompt-BmJjb"},"selected":false,"width":384,"height":337},{"id":"Prompt-polxO","type":"genericNode","position":{"x":1709.9593707440326,"y":1161.4212576598093},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":"Parsed Research Paper:\n\n{paper_text}","name":"template","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput"},"paper_text":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"paper_text","display_name":"paper_text","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["paper_text"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false},"id":"Prompt-polxO"},"selected":false,"width":384,"height":431},{"id":"ParseData-Lb2jD","type":"genericNode","position":{"x":1201.1329167993918,"y":1112.6108635262467},"data":{"type":"ParseData","node":{"template":{"_type":"Component","data":{"trace_as_input":true,"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"data","display_name":"Data","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The data to convert to text.","title_case":false,"type":"other","_input_type":"DataInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.io import DataInput, MultilineInput, Output, StrInput\nfrom langflow.schema.message import Message\n\n\nclass ParseDataComponent(Component):\n    display_name = \"Parse Data\"\n    description = \"Convert Data into plain text following a specified template.\"\n    icon = \"braces\"\n    name = \"ParseData\"\n\n    inputs = [\n        DataInput(name=\"data\", display_name=\"Data\", info=\"The data to convert to text.\"),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.\",\n            value=\"{text}\",\n        ),\n        StrInput(name=\"sep\", display_name=\"Separator\", advanced=True, value=\"\\n\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"parse_data\"),\n    ]\n\n    def parse_data(self) -> Message:\n        data = self.data if isinstance(self.data, list) else [self.data]\n        template = self.template\n\n        result_string = data_to_text(template, data, sep=self.sep)\n        self.status = result_string\n        return Message(text=result_string)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"sep":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"\n","name":"sep","display_name":"Separator","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"StrInput"},"template":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"{text}","name":"template","display_name":"Template","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Convert Data into plain text following a specified template.","icon":"braces","base_classes":["Message"],"display_name":"Parse Data","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"parse_data","value":"__UNDEFINED__","cache":true}],"field_order":["data","template","sep"],"beta":false,"edited":false},"id":"ParseData-Lb2jD"},"selected":false,"width":384,"height":401},{"id":"base64 File-gNcQS","type":"genericNode","position":{"x":662.4798684352011,"y":992.1170697179748},"data":{"type":"base64 File","node":{"template":{"_type":"Component","base64_string":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":true,"placeholder":"Enter base64 encoded file string here...","show":true,"value":"","name":"base64_string","display_name":"base64 Encoded File","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import base64\r\nimport os\r\nimport tempfile\r\nfrom langflow.custom import Component\r\nfrom langflow.schema import Data\r\nfrom langflow.base.data.utils import parse_text_file_to_data\r\nfrom langflow.io import BoolInput, MessageTextInput, Output\r\n\r\nclass Base64FileLoader(Component):\r\n    display_name = \"base64 File\"\r\n    description = \"Loads a file from a base64 encoded string, temporarily stores it, and processes similar to FileComponent.\"\r\n    icon = \"file-text\"\r\n    name = \"base64 File\"\r\n\r\n    inputs = [\r\n        MessageTextInput(name=\"base64_string\", display_name=\"base64 Encoded File\", required=True, placeholder=\"Enter base64 encoded file string here...\"),\r\n        MessageTextInput(name=\"filename\", display_name=\"Filename\", required=True, placeholder=\"Enter filename with extension...\"),\r\n        BoolInput(\r\n            name=\"silent_errors\",\r\n            display_name=\"Silent Errors\",\r\n            advanced=True,\r\n            info=\"If true, errors will not raise an exception.\",\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(display_name=\"Data\", name=\"data\", method=\"decode_and_load_file\"),\r\n    ]\r\n\r\n    def decode_and_load_file(self) -> Data:\r\n\r\n        if not self.base64_string:\r\n            raise ValueError(\"Base64 string is empty. Please provide a valid base64 encoded file.\")\r\n        if not self.filename:\r\n            raise ValueError(\"Filename is required.\")\r\n\r\n        base64_val = self.base64_string\r\n        filename_val = self.filename \r\n        \r\n        # Decode the base64 string to bytes\r\n        try:\r\n            file_bytes = base64.b64decode(base64_val)\r\n        except Exception as e:\r\n            raise ValueError(f\"Failed to decode base64 string: {str(e)}\")\r\n\r\n        # Create a temporary directory to store the file\r\n        with tempfile.TemporaryDirectory() as tmpdirname:\r\n            temp_file_path = os.path.join(tmpdirname, filename_val)\r\n\r\n            # Write bytes to a temporary file\r\n            with open(temp_file_path, 'wb') as temp_file:\r\n                temp_file.write(file_bytes)\r\n\r\n            # Use the existing utility to parse the text file\r\n            try:\r\n                data = parse_text_file_to_data(temp_file_path, silent_errors=self.silent_errors)\r\n            except Exception as e:\r\n                raise ValueError(f\"Failed to parse the file: {str(e)}\")\r\n\r\n            # Ensure file is deleted after processing if required\r\n            os.remove(temp_file_path)\r\n\r\n            self.status = \"File processed successfully.\"\r\n            return data\r\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"filename":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":true,"placeholder":"Enter filename with extension...","show":true,"value":"","name":"filename","display_name":"Filename","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"},"silent_errors":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"silent_errors","display_name":"Silent Errors","advanced":true,"dynamic":false,"info":"If true, errors will not raise an exception.","title_case":false,"type":"bool","_input_type":"BoolInput"}},"description":"Loads a file from a base64 encoded string, temporarily stores it, and processes similar to FileComponent.","icon":"file-text","base_classes":["Data"],"display_name":"base64 File","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"data","display_name":"Data","method":"decode_and_load_file","value":"__UNDEFINED__","cache":true}],"field_order":["base64_string","filename","silent_errors"],"beta":false,"edited":true,"official":false},"id":"base64 File-gNcQS"},"selected":false,"width":384,"height":467,"positionAbsolute":{"x":662.4798684352011,"y":992.1170697179748},"dragging":false},{"id":"TextInput-AlrlI","type":"genericNode","position":{"x":92.29661360789726,"y":1366.4538119353083},"data":{"type":"TextInput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as input.","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Get text inputs from the Playground.","icon":"type","base_classes":["Message"],"display_name":"Text Input","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"edited":false},"id":"TextInput-AlrlI"},"selected":false,"width":384,"height":317,"positionAbsolute":{"x":92.29661360789726,"y":1366.4538119353083},"dragging":false},{"id":"TextInput-eYtbq","type":"genericNode","position":{"x":94.03771473448944,"y":835.4179683246732},"data":{"type":"TextInput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as input.","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Get text inputs from the Playground.","icon":"type","base_classes":["Message"],"display_name":"Text Input","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"edited":false},"id":"TextInput-eYtbq"},"selected":false,"width":384,"height":317,"positionAbsolute":{"x":94.03771473448944,"y":835.4179683246732},"dragging":false}],"edges":[{"source":"GoogleGenerativeAIModel-YZ8QZ","target":"TextOutput-vdHQL","sourceHandle":"{œdataTypeœ:œGoogleGenerativeAIModelœ,œidœ:œGoogleGenerativeAIModel-YZ8QZœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œTextOutput-vdHQLœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","id":"reactflow__edge-GoogleGenerativeAIModel-YZ8QZ{œdataTypeœ:œGoogleGenerativeAIModelœ,œidœ:œGoogleGenerativeAIModel-YZ8QZœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-TextOutput-vdHQL{œfieldNameœ:œinput_valueœ,œidœ:œTextOutput-vdHQLœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"TextOutput-vdHQL","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"GoogleGenerativeAIModel","id":"GoogleGenerativeAIModel-YZ8QZ","name":"text_output","output_types":["Message"]}},"selected":false,"className":""},{"source":"Prompt-BmJjb","target":"GoogleGenerativeAIModel-YZ8QZ","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-BmJjbœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-YZ8QZœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","id":"reactflow__edge-Prompt-BmJjb{œdataTypeœ:œPromptœ,œidœ:œPrompt-BmJjbœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-YZ8QZ{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-YZ8QZœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"system_message","id":"GoogleGenerativeAIModel-YZ8QZ","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-BmJjb","name":"prompt","output_types":["Message"]}},"selected":false,"className":""},{"source":"Prompt-polxO","target":"GoogleGenerativeAIModel-YZ8QZ","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-polxOœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-YZ8QZœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","id":"reactflow__edge-Prompt-polxO{œdataTypeœ:œPromptœ,œidœ:œPrompt-polxOœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-YZ8QZ{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-YZ8QZœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"GoogleGenerativeAIModel-YZ8QZ","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-polxO","name":"prompt","output_types":["Message"]}},"selected":false,"className":""},{"source":"ParseData-Lb2jD","target":"Prompt-polxO","sourceHandle":"{œdataTypeœ:œParseDataœ,œidœ:œParseData-Lb2jDœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","targetHandle":"{œfieldNameœ:œpaper_textœ,œidœ:œPrompt-polxOœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","id":"reactflow__edge-ParseData-Lb2jD{œdataTypeœ:œParseDataœ,œidœ:œParseData-Lb2jDœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-polxO{œfieldNameœ:œpaper_textœ,œidœ:œPrompt-polxOœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"paper_text","id":"Prompt-polxO","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"ParseData","id":"ParseData-Lb2jD","name":"text","output_types":["Message"]}},"selected":false,"className":""},{"source":"base64 File-gNcQS","sourceHandle":"{œdataTypeœ:œbase64 Fileœ,œidœ:œbase64 File-gNcQSœ,œnameœ:œdataœ,œoutput_typesœ:[œDataœ]}","target":"ParseData-Lb2jD","targetHandle":"{œfieldNameœ:œdataœ,œidœ:œParseData-Lb2jDœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"data","id":"ParseData-Lb2jD","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"base64 File","id":"base64 File-gNcQS","name":"data","output_types":["Data"]}},"id":"reactflow__edge-base64 File-gNcQS{œdataTypeœ:œbase64 Fileœ,œidœ:œbase64 File-gNcQSœ,œnameœ:œdataœ,œoutput_typesœ:[œDataœ]}-ParseData-Lb2jD{œfieldNameœ:œdataœ,œidœ:œParseData-Lb2jDœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}"},{"source":"TextInput-AlrlI","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-AlrlIœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"base64 File-gNcQS","targetHandle":"{œfieldNameœ:œfilenameœ,œidœ:œbase64 File-gNcQSœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"filename","id":"base64 File-gNcQS","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-AlrlI","name":"text","output_types":["Message"]}},"id":"reactflow__edge-TextInput-AlrlI{œdataTypeœ:œTextInputœ,œidœ:œTextInput-AlrlIœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-base64 File-gNcQS{œfieldNameœ:œfilenameœ,œidœ:œbase64 File-gNcQSœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"},{"source":"TextInput-eYtbq","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-eYtbqœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"base64 File-gNcQS","targetHandle":"{œfieldNameœ:œbase64_stringœ,œidœ:œbase64 File-gNcQSœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"base64_string","id":"base64 File-gNcQS","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-eYtbq","name":"text","output_types":["Message"]}},"id":"reactflow__edge-TextInput-eYtbq{œdataTypeœ:œTextInputœ,œidœ:œTextInput-eYtbqœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-base64 File-gNcQS{œfieldNameœ:œbase64_stringœ,œidœ:œbase64 File-gNcQSœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"}],"viewport":{"x":165.49019032569964,"y":-163.30030808397362,"zoom":0.4352752816480626}},"description":"Creates a digest version of a research paper","name":"ResearchPaperDigest","last_tested_version":"1.0.14","endpoint_name":"research-paper-digest","is_component":false}